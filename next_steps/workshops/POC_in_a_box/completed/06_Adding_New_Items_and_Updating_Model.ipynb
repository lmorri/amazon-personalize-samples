{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with Campaigns <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "In this notebook, you will deploy and interact with campaigns in Amazon Personalize.\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "1. [Create campaigns](#create)\n",
    "1. [Interact with campaigns](#interact)\n",
    "1. [Batch recommendations](#batch)\n",
    "1. [Wrap up](#wrapup)\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "At this point, you should have several solutions and at least one solution version for each. Once a solution version is created, it is possible to get recommendations from them, and to get a feel for their overall behavior.\n",
    "\n",
    "This notebook starts off by deploying each of the solution versions from the previous notebook into individual campaigns. Once they are active, there are resources for querying the recommendations, and helper functions to digest the output into something more human-readable. \n",
    "\n",
    "As you with your customer on Amazon Personalize, you can modify the helper functions to fit the structure of their data input files to keep the additional rendering working.\n",
    "\n",
    "To get started, once again, we need to import libraries, load values from previous notebooks, and load the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#import datetime\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "import boto3, botocore\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')\n",
    "\n",
    "# Establish a connection to Personalize's event streaming\n",
    "personalize_events = boto3.client(service_name='personalize-events')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add New items to Catalog <a class=\"anchor\" id=\"interact\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "Now that all campaigns are deployed and active, we will need to add content to our catalog as it is released. Given that this content is completely new, it will not have interactions in the dataset for the user personalization solution to train on. Due to this we will use the exploration capability in the user personalization campaign. \n",
    "\n",
    "First we will get the current time, which we will then use as the CREATION_TIMESTAMP for our new items. Amazon Personalize uses creation timestamp data (in UNIX epoch time format, in seconds) to calculate the age of an item and adjust recommendations accordingly.\n",
    "\n",
    "If creation timestamp data is missing for one or more items, Amazon Personalize infers this information from interaction data, if any, and uses the timestamp of the itemâ€™s oldest interaction data as the item's creation date. If an item has no interaction data, its creation date is set as the timestamp of the latest interaction in the training set and is considered a new item. \n",
    "\n",
    "The creation timestamp data will be added to the user personalization model (as long as exploration value is above 0 and the creation timestamp is within the exploration_item_age_cut_off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currenttime = round(time.time()) - 1000\n",
    "currentyear = datetime.utcfromtimestamp(currenttime).strftime('%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_titles = {'ITEM_ID': [itemmetadata_df.last_valid_index() + 1, itemmetadata_df.last_valid_index() + 2, itemmetadata_df.last_valid_index() + 3, itemmetadata_df.last_valid_index() + 4], 'title': ['New Title 1' + \" (\" + str(currentyear) + \")\", 'New Title 2' + \" (\" + str(currentyear) + \")\", 'New Title 3' + \" (\" + str(currentyear) + \")\", 'New Title 4' + \" (\" + str(currentyear) + \")\"],'GENRE': ['Action|Comedy', 'Sci-Fi|Fantasy', 'Drama|Thriller', 'Documentary|IMAX'],'YEAR': [currentyear, currentyear, currentyear, currentyear],'CREATION_TIMESTAMP': [round(time.time()) - 1000, round(time.time()) - 2000, round(time.time()) - 3000, round(time.time())]}\n",
    "new_titles_df = pd.DataFrame(new_titles)\n",
    "new_titles_df=new_titles_df.astype(dtype= {\"ITEM_ID\":\"int64\", \"title\":\"object\", \"GENRE\":\"object\",\"YEAR\":\"object\",\"CREATION_TIMESTAMP\":\"int64\"})\n",
    "new_titles_df=new_titles_df.set_index('ITEM_ID')\n",
    "new_titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemmetadata_df = itemmetadata_df.append(new_titles_df, ignore_index=False)\n",
    "itemmetadata_df = itemmetadata_df[['GENRE','YEAR','CREATION_TIMESTAMP']]\n",
    "itemmetadata_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add the new titles to the look up table from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_items_df = new_titles_df[['title']]\n",
    "new_items_df = new_items_df.rename_axis('movieId')\n",
    "items_df = items_df.append(new_items_df, ignore_index=False)\n",
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will add the titles to item metadata via a batch import process. Note, you can also add these in realtime using the putitems api (see code example below) \n",
    "\n",
    "```python\n",
    "def add_title(item_id, genre, year, creationtimestamp):\n",
    "   \n",
    "    itemId= str(item_id)\n",
    "    #print(itemId)\n",
    "    itemdata = {\n",
    "        \"genre\": genre,\n",
    "        \"year\": year,\n",
    "        \"creationTimestamp\": creationtimestamp\n",
    "    }\n",
    "# Make Call\n",
    "    personalize_events.put_items(\n",
    "    datasetArn=items_dataset_arn,\n",
    "    items=[\n",
    "        {\n",
    "            'itemId': str(itemId),\n",
    "            'properties': json.dumps(itemdata)\n",
    "        },\n",
    "    ]\n",
    "    )\n",
    "```\n",
    "and then execute by calling the function\n",
    "\n",
    "```python\n",
    "for index, row in new_titles_df.iterrows():\n",
    "    print(\"Adding Item #\" + str(index) + \" \" + row['title'] + \" with genres \" + row['genres'])\n",
    "    add_title(item_id=index,genre=row['genres'],year=row['year'],creationtimestamp=row['CREATION_TIMESTAMP'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to 02_Validating_and_Importing_Item_Metadata.ipynb we will store the updated item_metadata_df to a csv file and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemmetadata_filename = \"item-meta.csv\"\n",
    "itemmetadata_df.to_csv((data_dir+\"/\"+itemmetadata_filename), index=True, float_format='%.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemmetadata_file_path = data_dir + \"/\" + itemmetadata_filename\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(itemmetadata_filename).upload_file(itemmetadata_file_path)\n",
    "interactions_s3DataPath = \"s3://\"+bucket_name+\"/\"+itemmetadata_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the updated file is in S3, we will start a dataset import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"personalize-poc-item-import\" + str(currenttime),\n",
    "    datasetArn = items_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, itemmetadata_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the status of the import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 6*60*60 # 6 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"DatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the model(solutionVersion), we ran the createSolutionVersion with trainingMode set to UPDATE. This updates the model with the latest item information and adjusts the exploration according to implicit feedback from the users. This is not equivalent to training a model, which you can do by setting trainingMode to FULL. You should perform full training less frequently, typically once every 1â€“5 days. When the new updated solutionVersion is created, you can update the campaign to get recommendations using it. NOTE: This happens automatically every 2 hours, for the purposes of this workshop we are running it on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_personalization_update_solution_response = personalize.create_solution_version(\n",
    "    solutionArn = user_personalization_solution_arn, \n",
    "    trainingMode='UPDATE')\n",
    "new_user_personalization_solution_version_arn = user_personalization_update_solution_response['solutionVersionArn']\n",
    "print(\"Creating solution version: {}\".format(new_user_personalization_solution_version_arn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 60*60 # 1 hour\n",
    "while time.time() < max_time:\n",
    "    describe_solution_version_response = personalize.describe_solution_version(\n",
    "    solutionVersionArn = new_user_personalization_solution_version_arn\n",
    "    )\n",
    "    status = describe_solution_version_response[\"solutionVersion\"][\"status\"]\n",
    "    print(\"SolutionVersion: {}\".format(status))\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "         break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the solution is updated with the new solution version, including item-metadata information about our new titles, we can update the campaign to use this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userpersonalization_update_campaign_response = personalize.update_campaign(campaignArn=userpersonalization_campaign_arn, solutionVersionArn=new_user_personalization_solution_version_arn)\n",
    "userpersonalization_campaign_arn = userpersonalization_update_campaign_response['campaignArn']\n",
    "print(json.dumps(userpersonalization_update_campaign_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the status of the solution version update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_campaign_update_response = personalize.describe_campaign(\n",
    "            campaignArn = userpersonalization_campaign_arn)\n",
    "status = describe_campaign_update_response[\"campaign\"][\"status\"]\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the campaign itself is ACTIVE, however the latestCampaignUpdate is either in CREATE PENDING or CREATE IN_PROGRESS, the campaign will remain active with the previous solution version until the new solution version is active at which point it will become active in the current campaign. We will watch for the latestCampaignUpdate to become ACTIVE. Our new items will become available once the campaign is active with the new solution version. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 60*60 # 1 hour\n",
    "while time.time() < max_time:\n",
    "    describe_campaign_update_response = personalize.describe_campaign(\n",
    "            campaignArn = userpersonalization_campaign_arn\n",
    "        )\n",
    "    status = describe_campaign_update_response[\"campaign\"][\"latestCampaignUpdate\"][\"status\"]\n",
    "    print(\"CampaignUpdate: {}\".format(status))\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "         break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a movie lookup function as in the '05_Interacting_with_Campaigns_and_Filters.ipynb' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_by_id(movie_id, movie_df=items_df):\n",
    "    \"\"\"\n",
    "    This takes in an movie_id from Personalize so it will be a string,\n",
    "    converts it to an int, and then does a lookup in a default or specified\n",
    "    dataframe.\n",
    "    \n",
    "    A really broad try/except clause was added in case anything goes wrong.\n",
    "    \n",
    "    Feel free to add more debugging or filtering here to improve results if\n",
    "    you hit an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return movie_df.loc[int(movie_id)]['title']\n",
    "    except:\n",
    "        return \"Error obtaining title\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test a few simple values to check our error catching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A known good id (NewTitle4)\n",
    "print(get_movie_by_id(movie_id=str(items_df.last_valid_index())))\n",
    "# A bad type of value\n",
    "print(get_movie_by_id(movie_id='987.9393939'))\n",
    "# Really bad values\n",
    "print(get_movie_by_id(movie_id='Steve'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a way of rendering results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update DF rendering\n",
    "pd.set_option('display.max_rows', 30)\n",
    "\n",
    "def get_new_recommendations_df(recommendations_df, movie_ID):\n",
    "    # Get the movie name\n",
    "    movie_name = get_movie_by_id(movie_ID)\n",
    "    # Get the recommendations\n",
    "    get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "        campaignArn = sims_campaign_arn,\n",
    "        itemId = str(movie_ID),\n",
    "    )\n",
    "    # Build a new dataframe of recommendations\n",
    "    item_list = get_recommendations_response['itemList']\n",
    "    recommendation_list = []\n",
    "    for item in item_list:\n",
    "        movie = get_movie_by_id(item['itemId'])\n",
    "        recommendation_list.append(movie)\n",
    "    new_rec_DF = pd.DataFrame(recommendation_list, columns = [movie_name])\n",
    "    # Add this dataframe to the old one\n",
    "    recommendations_df = pd.concat([recommendations_df, new_rec_DF], axis=1)\n",
    "    return recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Personalization\n",
    "\n",
    "HRNN is one of the more advanced algorithms provided by Amazon Personalize, and is the underlying algorithm in the user personalization recipe. It supports personalization of the items for a specific user based on their past behavior and can intake real time events in order to alter recommendations for a user without retraining. \n",
    "\n",
    "Since HRNN relies on having a sampling of users, let's load the data we need for that and select 3 random users. Since Movielens does not include user data, we will select 3 random numbers from the range of user id's in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_FULL_MOVIELENS:\n",
    "    users = random.sample(range(1, 600), 10)\n",
    "else:\n",
    "    users = random.sample(range(1, 162000), 10)\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we render the recommendations for our 3 random users from above. After that, we will explore real-time interactions before moving on to Personalized Ranking.\n",
    "\n",
    "Again, we create a helper function to render the results in a nice dataframe.\n",
    "\n",
    "#### API call results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update DF rendering\n",
    "pd.set_option('display.max_rows', 30)\n",
    "\n",
    "def get_new_recommendations_df_users(recommendations_df, user_id):\n",
    "    # Get the movie name\n",
    "    #movie_name = get_movie_by_id(artist_ID)\n",
    "    # Get the recommendations\n",
    "    get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "        campaignArn = userpersonalization_campaign_arn,\n",
    "        userId = str(user_id),\n",
    "    )\n",
    "    # Build a new dataframe of recommendations\n",
    "    item_list = get_recommendations_response['itemList']\n",
    "    recommendation_list = []\n",
    "    for item in item_list:\n",
    "        movie = get_movie_by_id(item['itemId'])\n",
    "        recommendation_list.append(movie)\n",
    "    #print(recommendation_list)\n",
    "    new_rec_DF = pd.DataFrame(recommendation_list, columns = [user_id])\n",
    "    # Add this dataframe to the old one\n",
    "    recommendations_df = pd.concat([recommendations_df, new_rec_DF], axis=1)\n",
    "    return recommendations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df_users = pd.DataFrame()\n",
    "#users = users_df.sample(3).index.tolist()\n",
    "\n",
    "for user in users:\n",
    "    recommendations_df_users = get_new_recommendations_df_users(recommendations_df_users, user)\n",
    "\n",
    "recommendations_df_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the cold items are being recommended based on the exploration weight set when we created the campaign.Now lets send in some interactions from the sample users to simulate viewers engaging with this cold start content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dict = {}\n",
    "\n",
    "def send_movie_click(USER_ID, ITEM_ID, EVENT_TYPE):\n",
    "    \"\"\"\n",
    "    Simulates a click as an envent\n",
    "    to send an event to Amazon Personalize's Event Tracker\n",
    "    \"\"\"\n",
    "    # Configure Session\n",
    "    try:\n",
    "        session_ID = session_dict[str(USER_ID)]\n",
    "    except:\n",
    "        session_dict[str(USER_ID)] = str(uuid.uuid1())\n",
    "        session_ID = session_dict[str(USER_ID)]\n",
    "        \n",
    "    # Configure Properties:\n",
    "    event = {\n",
    "    \"itemId\": str(ITEM_ID),\n",
    "    }\n",
    "    event_json = json.dumps(event)\n",
    "        \n",
    "    # Make Call\n",
    "    \n",
    "    response = personalize_events.put_events(\n",
    "    trackingId = TRACKING_ID,\n",
    "    userId= str(USER_ID),\n",
    "    sessionId = session_ID,\n",
    "    eventList = [{\n",
    "        'sentAt': int(time.time()),\n",
    "        'eventType': str(EVENT_TYPE),\n",
    "        'properties': event_json\n",
    "        }]\n",
    "    )\n",
    "   \n",
    "def get_new_recommendations_df_users_real_time(recommendations_df, user_id, item_id, event_type):\n",
    "    # Get the artist name (header of column)\n",
    "    movie_name = get_movie_by_id(item_id)\n",
    "    # Interact with different movies\n",
    "    print('sending event ' + event_type + ' for ' + get_movie_by_id(item_id))\n",
    "    send_movie_click(USER_ID=user_id, ITEM_ID=item_id, EVENT_TYPE=event_type)\n",
    "    # Get the recommendations (note you should have a base recommendation DF created before)\n",
    "    get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "        campaignArn = userpersonalization_campaign_arn,\n",
    "        userId = str(user_id),\n",
    "    )\n",
    "    # Build a new dataframe of recommendations\n",
    "    item_list = get_recommendations_response['itemList']\n",
    "    recommendation_list = []\n",
    "    for item in item_list:\n",
    "        artist = get_movie_by_id(item['itemId'])\n",
    "        recommendation_list.append(artist)\n",
    "    new_rec_DF = pd.DataFrame(recommendation_list, columns = [movie_name])\n",
    "    # Add this dataframe to the old one\n",
    "    #recommendations_df = recommendations_df.join(new_rec_DF)\n",
    "    recommendations_df = pd.concat([recommendations_df, new_rec_DF], axis=1)\n",
    "    return recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will iterate through all of the sammple users and both click on and watch the 4 new titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this will take about 20 seconds to complete due to the sleeps\n",
    "for user in users:\n",
    "    print ('user ' + str(user))\n",
    "    for index, row in new_titles_df.iterrows():\n",
    "        user_id=user\n",
    "        print('sending event click for ' + get_movie_by_id(index))\n",
    "        send_movie_click(user_id, index,'click')\n",
    "        print('sending event watch for ' + get_movie_by_id(index))\n",
    "        send_movie_click(user_id, index,'watch')\n",
    "        sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply item filters to see recommendations for one of these users within a genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_FULL_MOVIELENS:\n",
    "    users = random.sample(range(1, 600), 3)\n",
    "else:\n",
    "    users = random.sample(range(1, 162000), 3)\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df_users = pd.DataFrame()\n",
    "#users = users_df.sample(3).index.tolist()\n",
    "\n",
    "for user in users:\n",
    "    recommendations_df_users = get_new_recommendations_df_users(recommendations_df_users, user)\n",
    "\n",
    "recommendations_df_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the recommendations for movies within a given genre. Within a VOD application you could create Shelves (also known as rails or carosels) easily by using these filters. Depending on the information you have about your items, You could also filter on additional information such as keyword, year/decade etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up <a class=\"anchor\" id=\"wrapup\"></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "With that you now have a fully working collection of models to tackle various recommendation and personalization scenarios, as well as the skills to manipulate customer data to better integrate with the service, and a knowledge of how to do all this over APIs and by leveraging open source data science tools.\n",
    "\n",
    "Use these notebooks as a guide to getting started with your customers for POCs. As you find missing components, or discover new approaches, cut a pull request and provide any additional helpful components that may be missing from this collection.\n",
    "\n",
    "You'll want to make sure that you clean up all of the resources deployed during this POC. We have provided a separate notebook which shows you how to identify and delete the resources in `07_Clean_Up.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
